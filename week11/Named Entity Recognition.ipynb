{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "In this notebook, we'll implement a simple named entity recognition (NER) tagger model and evaluate it on the [CoNLL 2003 shared task](http://www.cnts.ua.ac.be/conll2003/ner/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vocabulary' from 'vocabulary.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils\n",
    "reload(utils)\n",
    "import vocabulary\n",
    "reload(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window-based Tagger Model\n",
    "\n",
    "For our model, we'll implement a simple window-based model that predicts an entity tag based on the center word and its immediate neighbors.\n",
    "\n",
    "We'll use a two-layer neural network to do this. It's based on the \"Window approach network\" in Figure 1 of [Collobert et al. 2011](https://arxiv.org/pdf/1103.0398v1.pdf), and also quite similar to the NPLM model that [we studied in Week 4](../week4/Neural%20Probabilistic%20Language%20Model.ipynb).\n",
    "\n",
    "Here's a quick sketch, for a window size of $C = 3$:  \n",
    "![windowtagger.png](windowtagger.png)\n",
    "\n",
    "As in week 4, our model will have three parts. Let $k = (C-1)/2$:\n",
    "\n",
    "- **Embedding layer:** $ f^{(i')} = W_{in}[w^{(i')}] $ for each window element $i' \\in \\{i-k,\\ldots, i, \\ldots, i+k\\}$, and concatenated embeddings $x^{(i)} = [f^{(i-k)},\\ldots,f^{(i+k)}]$\n",
    "- **Hidden layer:** $ h^{(i)} = \\tanh(x^{(i)} W_h + b_h) $\n",
    "- **Output layer:** $\\hat{P}(y^{(i)}) = \\text{softmax}(h^{(i)} W_{out} + b_{out}) $\n",
    "\n",
    "Note that as in Week 4 and Assignment 1, we write left multiplication to be more consistent with the TensorFlow implementation.\n",
    "\n",
    "Our model hyperparameters are:\n",
    "- `V`: vocabulary size\n",
    "- `M`: embedding size\n",
    "- `C`: window size\n",
    "- `H`: hidden size\n",
    "- `num_classes`: number of output classes\n",
    "\n",
    "For CoNLL 2003, `num_classes = 5`: the entity types `PER`, `ORG`, `LOC`, and `MISC`, and the \"other\" tag `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "##\n",
    "# Hyperparameters\n",
    "V = 30000\n",
    "M = 50\n",
    "C = 5\n",
    "H = 100\n",
    "num_classes = 5\n",
    "\n",
    "k = int((C - 1) / 2)\n",
    "assert(C == 2*k + 1)\n",
    "\n",
    "with tf.name_scope(\"Inputs\"):\n",
    "  w_ = tf.placeholder(tf.int32, shape=[None, C], name=\"w\")\n",
    "  y_ = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.variable_scope(\"Embedding_Layer\"):\n",
    "  W_in_ = tf.get_variable(\"W_in\", shape=[V,M], dtype=tf.float32, \n",
    "                          initializer=tf.random_uniform_initializer(-1, 1))\n",
    "  f_ = tf.nn.embedding_lookup(W_in_, w_, name=\"f\")\n",
    "  # Reshape as a simple way to concatenate embeddings\n",
    "  x_ = tf.reshape(f_, [-1, C*M], name=\"x\")\n",
    "  \n",
    "with tf.variable_scope(\"Hidden_Layer\"):\n",
    "  W_h_ = tf.get_variable(\"W_h\", shape=[C*M, H], dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "  b_h_ = tf.get_variable(\"b_h\", dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer([H]))\n",
    "  h_ = tf.tanh(tf.matmul(x_, W_h_) + b_h_, name=\"h\")\n",
    "  \n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "  W_out_ = tf.get_variable(\"W_out\", shape=[H, num_classes], dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "  b_out_ = tf.get_variable(\"b_out\", dtype=tf.float32, \n",
    "                           initializer=tf.zeros_initializer([num_classes]))\n",
    "  logits_ = tf.add(tf.matmul(h_, W_out_), b_out_, name=\"logits\")\n",
    "\n",
    "with tf.name_scope(\"Loss_Function\"):\n",
    "  point_loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_, y_)\n",
    "  loss_ = tf.reduce_sum(point_loss_)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "  alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "  optimizer_ = tf.train.AdagradOptimizer(alpha_)\n",
    "  train_step_ = optimizer_.minimize(loss_)\n",
    "\n",
    "with tf.name_scope(\"Prediction\"):\n",
    "  pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "  pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    \n",
    "# Initializer step\n",
    "init_ = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can inspect our model in TensorBoard. Run the cell below, then in a separate terminal, run:\n",
    "```\n",
    "tensorboard --logdir=\"~/w266/week11/tf_summaries\" --port 6006\n",
    "```\n",
    "and go to http://localhost:6006/#graphs\n",
    "\n",
    "It should look something like this:\n",
    "![graph](graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                        tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "We've provided a processed version of the English part of the [CoNLL 2003 dataset](http://www.cnts.ua.ac.be/conll2003/ner/), located in `data/ner/train` and `data/ner/dev`. As with many NLP datasets, the text is from newswire articles - in this case, a portion of the [Reuters corpus](http://trec.nist.gov/data/reuters/reuters.html). The training set consists of about 200k words with 30k entities.\n",
    "\n",
    "The format looks like this (eew, tabs):\n",
    "```\n",
    "-DOCSTART-\tO\n",
    "\n",
    "EU\tORG\n",
    "rejects\tO\n",
    "German\tMISC\n",
    "call\tO\n",
    "to\tO\n",
    "boycott\tO\n",
    "British\tMISC\n",
    "lamb\tO\n",
    ".\tO\n",
    "```\n",
    "In a real model, we need to use BIO or similar encoding to handle adjacent entities properly; for this example, however, we'll just predict the raw rags.\n",
    "\n",
    "We've provided some code in `utils.py` to read this; take a look at that file for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ORG': 2, 'MISC': 4, 'PER': 1, 'O': 0, 'LOC': 3}\n",
      "Train set: 203621 rows with 34043 non-O tags\n",
      "Dev set: 51362 rows with 8603 non-O tags\n"
     ]
    }
   ],
   "source": [
    "tags = [\"O\", \"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "tag_to_num = {t:i for i,t in enumerate(tags)}\n",
    "num_to_tag = {i:t for i,t in enumerate(tags)}\n",
    "print tag_to_num\n",
    "\n",
    "# Load to list( list( (word, tag) ) )\n",
    "train_docs = utils.load_dataset(\"data/ner/train\")\n",
    "dev_docs = utils.load_dataset(\"data/ner/dev\")\n",
    "\n",
    "# Build vocabulary\n",
    "word_stream = utils.canonicalize_words(utils.flatten([w for w,t in doc] \n",
    "                                                     for doc in train_docs))\n",
    "vocab = vocabulary.Vocabulary(word_stream, size=V)\n",
    "\n",
    "# Load to windows\n",
    "train_w, train_y = utils.docs_to_windows(train_docs, tag_to_num, vocab, k)\n",
    "dev_w, dev_y = utils.docs_to_windows(dev_docs, tag_to_num, vocab, k)\n",
    "\n",
    "print \"Train set: %d rows with\" % len(train_w),\n",
    "print \"%d non-O tags\" % sum(train_y != tag_to_num[\"O\"])\n",
    "print \"Dev set: %d rows with\" % len(dev_w),\n",
    "print \"%d non-O tags\" % sum(dev_y != tag_to_num[\"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of the loaded data, converting the indices back to strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_{i-2}</th>\n",
       "      <th>w_{i-1}</th>\n",
       "      <th>w_{i}</th>\n",
       "      <th>w_{i+1}</th>\n",
       "      <th>w_{i+2}</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>948</td>\n",
       "      <td>12151</td>\n",
       "      <td>198</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>948</td>\n",
       "      <td>12151</td>\n",
       "      <td>198</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>948</td>\n",
       "      <td>12151</td>\n",
       "      <td>198</td>\n",
       "      <td>590</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12151</td>\n",
       "      <td>198</td>\n",
       "      <td>590</td>\n",
       "      <td>10</td>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198</td>\n",
       "      <td>590</td>\n",
       "      <td>10</td>\n",
       "      <td>3989</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w_{i-2}  w_{i-1}  w_{i}  w_{i+1}  w_{i+2}  y\n",
       "0        0        0    948    12151      198  2\n",
       "1        0      948  12151      198      590  0\n",
       "2      948    12151    198      590       10  4\n",
       "3    12151      198    590       10     3989  0\n",
       "4      198      590     10     3989      207  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ <s> <s> eu rejects german ]- ORG\n",
      "[ <s> eu rejects german call ]- O\n",
      "[ eu rejects german call to ]- MISC\n",
      "[ rejects german call to boycott ]- O\n",
      "[ german call to boycott british ]- O\n"
     ]
    }
   ],
   "source": [
    "cols = [\"w_{i-2}\", \"w_{i-1}\", \"w_{i}\", \"w_{i+1}\", \"w_{i+2}\", \"y\"]\n",
    "utils.pretty_print_matrix(np.hstack([train_w[:5], \n",
    "                                     train_y[:5].reshape([-1,1])]), \n",
    "                          cols=cols, dtype=int)\n",
    "\n",
    "def show_window(w, y):\n",
    "  return (\"[ %s ]\" % \" \".join(vocab.ids_to_words(w)) \n",
    "          + \"- %s\" % num_to_tag[y])\n",
    "  \n",
    "for i in range(5):\n",
    "  print show_window(train_w[i], train_y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Model\n",
    "\n",
    "With our data in array form, we can train our model much like any machine learning model. The code below is almost identical to the Week 4 NPLM notebook.\n",
    "\n",
    "We'll optimize cross-entropy loss as usual, but instead of measuring perplexity to evaluate performance, we'll measure F1 score on the non-O classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Helper functions for training, to reduce boilerplate code\n",
    "\n",
    "def train_batch(session, batch, alpha):\n",
    "    feed_dict = {w_:batch[0],\n",
    "                 y_:batch[1],\n",
    "                 alpha_:alpha}\n",
    "    c, _ = session.run([loss_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c\n",
    "\n",
    "def score_batch(session, batch):\n",
    "  feed_dict = {w_:batch[0],\n",
    "               y_:batch[1]}\n",
    "  return session.run([loss_, pred_max_], feed_dict=feed_dict)\n",
    "  \n",
    "def batch_generator(w, y, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\"\"\"\n",
    "    assert(len(w) == len(y))\n",
    "    for i in xrange(0, len(w), batch_size):\n",
    "        yield w[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some helpers to compute and measure our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Evaluation code for multi-class prediction\n",
    "from sklearn import metrics\n",
    "\n",
    "def eval_performance(y_true, y_pred, tagnames):\n",
    "  pre, rec, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "  avg_pre = (100*sum(pre[1:] * support[1:])/sum(support[1:]))\n",
    "  avg_rec = (100*sum(rec[1:] * support[1:])/sum(support[1:]))\n",
    "  avg_f1 = (100*sum(f1[1:] * support[1:])/sum(support[1:]))\n",
    "  print \"mean P, R, F1: %.02f%% / %.02f%% / %.02f%%\" % (avg_pre, avg_rec, avg_f1)\n",
    "    \n",
    "def pred_dataset(session, w, y):\n",
    "  y_pred = []\n",
    "  for batch in batch_generator(w, y, batch_size=1000):\n",
    "    _, yp = score_batch(session, batch)\n",
    "    y_pred.extend(yp)\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll train our model and evaluate on the dev set! The model should train very quickly, less than a minute or so with the default parameters - although this depends somewhat on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 1000 minibatches\n",
      "[epoch 1] seen 2000 minibatches\n",
      "[epoch 1] Completed 2036 minibatches in 0:00:01\n",
      "[epoch 1] Average cost: 0.256\n",
      "Train: mean P, R, F1: 89.56% / 82.33% / 85.55%\n",
      "Dev:   mean P, R, F1: 85.31% / 65.88% / 74.07%\n",
      "\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 1000 minibatches\n",
      "[epoch 2] seen 2000 minibatches\n",
      "[epoch 2] Completed 2036 minibatches in 0:00:01\n",
      "[epoch 2] Average cost: 0.094\n",
      "Train: mean P, R, F1: 95.56% / 93.17% / 94.34%\n",
      "Dev:   mean P, R, F1: 87.84% / 69.71% / 77.29%\n",
      "\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 1000 minibatches\n",
      "[epoch 3] seen 2000 minibatches\n",
      "[epoch 3] Completed 2036 minibatches in 0:00:01\n",
      "[epoch 3] Average cost: 0.047\n",
      "Train: mean P, R, F1: 97.70% / 96.90% / 97.30%\n",
      "Dev:   mean P, R, F1: 87.93% / 71.53% / 78.38%\n",
      "\n",
      "Dev set classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.99      0.97     42759\n",
      "        PER       0.96      0.64      0.77      3149\n",
      "        ORG       0.75      0.70      0.72      2092\n",
      "        LOC       0.90      0.84      0.87      2094\n",
      "       MISC       0.86      0.73      0.79      1268\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One epoch = one pass through the training data\n",
    "num_epochs = 3\n",
    "batch_size = 100\n",
    "alpha = 0.1  # learning rate\n",
    "print_every = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_)\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in xrange(1,num_epochs+1):\n",
    "  t0_epoch = time.time()\n",
    "  epoch_cost = 0.0\n",
    "  print \"\"\n",
    "  for i, batch in enumerate(batch_generator(train_w, train_y, batch_size)):\n",
    "      if (i % print_every == 0):\n",
    "          print \"[epoch %d] seen %d minibatches\" % (epoch, i)\n",
    "\n",
    "      epoch_cost += train_batch(session, batch, alpha)\n",
    "\n",
    "  avg_cost = epoch_cost / len(train_w)\n",
    "  print \"[epoch %d] Completed %d minibatches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch))\n",
    "  print \"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,)\n",
    "\n",
    "  ##\n",
    "  # Evaluate on train and dev\n",
    "  print \"Train:\",\n",
    "  train_pred = pred_dataset(session, train_w, train_y)\n",
    "  eval_performance(train_y, train_pred, tags)\n",
    "  print \"Dev:  \",\n",
    "  dev_pred = pred_dataset(session, dev_w, dev_y)\n",
    "  eval_performance(dev_y, dev_pred, tags)\n",
    "  \n",
    "##\n",
    "# Full report on dev set\n",
    "print \"\"\n",
    "print \"Dev set classification report:\"\n",
    "dev_pred = pred_dataset(session, dev_w, dev_y)\n",
    "print metrics.classification_report(dev_y, dev_pred,\n",
    "                                    target_names=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F1 are defined as usual; \"support\" is just the total number of targets of that class (i.e. our dev set has 3149 words tagged as `PER` out of 51362 total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises (not graded)\n",
    "\n",
    "**1.** What are the dimensions of $W_h$ in this model, if $C = 3$, $M = 100$, and $H = 100$?\n",
    "\n",
    "**2.** Would this model benefit from pre-trained word embeddings? Why or why not?\n",
    "\n",
    "**3.** The training set has only about 24k unique words. If you use pre-trained word embeddings with a vocabulary of 300,000, should you allow the embeddings to change while training the NER tagger, or hold them fixed? How might your choice affect the ability of your model to generalize?\n",
    "\n",
    "**4.** For simplicity, this demo maps all words to lowercase (see `utils.canonicalize_word`). Why might this be a bad idea for named entity recognition?\n",
    "\n",
    "**5.** Does the model above overfit or underfit? How might you adjust it to improve performance?\n",
    "\n",
    "**6.** `ORG` seems to be the lowest-performing class, based on the report above. Look at the misclassified examples - can you guess why the model might be having a hard time with this class? What features or modifications of the model might improve performance here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ victory while kent made up ]- ORG   (pred: PER)\n",
      "[ after bowling somerset out for ]- ORG   (pred: O)\n",
      "[ oval , surrey captain chris ]- ORG   (pred: LOC)\n",
      "[ <s> <s> derbyshire kept up ]- ORG   (pred: PER)\n",
      "[ , took derbyshire to DGDGDG ]- ORG   (pred: O)\n",
      "[ to <unk> nottinghamshire for DGDGDG ]- ORG   (pred: O)\n",
      "[ DGDGDG-DG , derbyshire DGDGDG ( ]- ORG   (pred: PER)\n",
      "[ , the test and county ]- ORG   (pred: O)\n",
      "[ the test and county cricket ]- ORG   (pred: O)\n",
      "[ another against british universities , ]- ORG   (pred: MISC)\n",
      "[ against the minor counties and ]- ORG   (pred: MISC)\n",
      "[ the minor counties and </s> ]- ORG   (pred: O)\n",
      "[ DGDG v <unk> of <unk> ]- ORG   (pred: O)\n",
      "[ v <unk> of <unk> 's ]- ORG   (pred: O)\n",
      "[ gloucestershire or sussex or surrey ]- ORG   (pred: O)\n",
      "[ soccer - rotor fans locked ]- ORG   (pred: LOC)\n",
      "[ stones at dynamo moscow players ]- ORG   (pred: LOC)\n",
      "[ at dynamo moscow players during ]- ORG   (pred: LOC)\n",
      "[ friday that rotor would play ]- ORG   (pred: O)\n",
      "[ would play lada togliatti to ]- ORG   (pred: O)\n"
     ]
    }
   ],
   "source": [
    "# Get indices of misclassified \"ORG\" targets\n",
    "mask = (dev_y == tag_to_num['ORG']) & (dev_y != dev_pred)\n",
    "idxs = np.arange(len(dev_y))[mask]\n",
    "for i in idxs[:20]:\n",
    "  print show_window(dev_w[i], dev_y[i]),\n",
    "  print \"  (pred: %s)\" % num_to_tag[dev_pred[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
